#!/usr/bin/env bash
# Default configuration
# shellcheck disable=all
# Project config values
PROJECT="Rosary In A Year (RIAY)"
# Turn on or turn off logging
LOGGING=false
# Set log level
# takes values notset, debug, info, warning, error and critical
LOGGING_LEVEL=info
# The year in which the podcasts are being followed
# This ensures that the day of the week is aligned with the year
YEAR=2025

# commands file
COMMANDS_FILE="commands.txt"

# Github config values
REPO_OWNER=linusjf
REPO_NAME=RIAY

# overlay icon configuration
ICON_FILE="play-button.png"
ICON_SIZE="256x256"
ICON_OFFSET="+32+0"
ICON_COMMENT="Play Icon Added"

# add videos configuration
COMPACT_FILE="compact.txt"
VIDEOS_FILE="videos.txt"

# curl config values
# Time between successive requests to the LLM models
# This is to minimize triggering of rate limiting
GAP_BW_REQS=0
# Maximum number of retries for a REST API call
CURL_MAX_RETRIES=5
# The initial retry delay of 2 seconds which increases exponentially for each retry
CURL_INITIAL_RETRY_DELAY=2
# In curl, the --connect-timeout option sets how long (in seconds) to wait for the connection phase to the server to complete.
CURL_CONNECT_TIMEOUT=60
# --max-time in curl sets the maximum total time (in seconds) that the whole curl operation (connect + download/upload) is allowed to take.
CURL_MAX_TIME=600
# --the status codes for which retry can be attempted
CURL_RETRY_STATUS_CODES=(408 429 500 502 503 504)
# -- the status codes for which retry headers are to be read
CURL_RETRY_HEADER_STATUS_CODES=(429 503)

# Transcription config values
# whether to transcribe videos using a transcription service or rely on youtube auto-generated captions or creator provided subtitles
TRANSCRIBE_VIDEOS=false
# whether to transcribe locally or not
TRANSCRIBE_LOCALLY=false
# whether to use faster-whisper python library
USE_FASTER_WHISPER=true
# option whether to enable failover mode
ENABLE_FAILOVER_MODE=true

# Automatic Speech Recognition (ASR) LLM config values
# ASR API KEY
ASR_LLM_API_KEY="$DEEPINFRA_TOKEN"
# ASR LLM base url
ASR_LLM_BASE_URL="https://api.deepinfra.com/v1"
# ASR LLM endpoint
# use transcriptions end-point; faster than translations which requires language detection
ASR_LLM_ENDPOINT="/openai/audio/transcriptions"
# ASR LLM model used for transcriptions and translations
ASR_LLM_MODEL="openai/whisper-large-v3"
# ASR LLM model used locally while running whisper via command line
# valid values: tiny, base, small, medium, large, turbo
# as the names suggest memory requirements and parameters used increase as per the model size
# accuracy also improves as the model size increases
# https://github.com/openai/whisper?tab=readme-ov-file#available-models-and-languages
# Note that running ASR locally is several magnitudes slower than executing the web service
# the small model works best with beam_size 5 for accurate transcriptions.
# If you can live with a little inaccuracy, such as poco a poco mistranscribed, it doesn't matter
# since it is omitted by the subsequent summarization prompts, you could use the base model with beam-size 3 for quicker
# local transcriptions. For accuracy, the minimal best config is small with beam size 5
ASR_LOCAL_MODEL="small"
# initial prompt for whisper
ASR_INITIAL_PROMPT="In Ascension Press' Rosary in a Year podcast, Fr. Mark-Mary Ames meditates with sacred art, saint writings, and scripture. Poco a poco."
# carry initial prompt for whisper. whether the initial prompt should be used in decoding each segment
ASR_CARRY_INITIAL_PROMPT=true
# beam size for ASR
ASR_BEAM_SIZE=5

# youtube config values
# number of retries for yt-dlp when downloading captions
YT_DLP_RETRIES=20
# time to wait in seconds before giving up
YT_DLP_SOCKET_TIMEOUT=30
# captions output directory
CAPTIONS_OUTPUT_DIR="captions"

# AI Config values
# The temperature value to be set for the LLM models
# Set it to zero to ensure reproducibility for a model
TEMPERATURE=0.5

# text llm api key
TEXT_LLM_API_KEY="$DEEPSEEK_API_KEY"
# text llm base url
TEXT_LLM_BASE_URL="https://api.deepseek.com"
# text llm chat model endpoint
TEXT_LLM_CHAT_ENDPOINT="/chat/completions"
# text llm model used for summarization
TEXT_LLM_MODEL="deepseek-chat"

# The system prompt for summarizing text.
SYSTEM_SUMMARY_PROMPT="You are a helpful assistant that summarizes content. Be concise, helpful."
# The prompt for summarizing chunks within the podcast transcript.
CHUNK_SUMMARY_PROMPT="Summarize this text, excluding plugs, branding, and promotions. Avoid mention of Day, podcast and Rosary in a Day. Additionally, exclude repetitive prayers such as Our Father, Hail Mary and Glory Be. Retain details of artwork decribed including title, artist (full name), current location of artwork, medium, style, date (year, decade and century) and brief description."
# The prompt for the final summary of all the chunk summaries
FINAL_SUMMARY_PROMPT="Summarize the following text in the voice and style of C.S. Lewis, employing his clarity, moral insight, and rhetorical flair, as if writing directly to an intelligent but unassuming reader. Generate a title as well. Avoid modern jargon, maintain a tone of gentle conviction, and present the ideas as timeless truths. Do not include any meta-commentary, footnotes, or explanations. Retain details of artworks. Start with a level three markdown header
'### AI-Generated Summary: '. Append your generated title (no colons, proper grammar) to the header."
# The meta-prompt to generate image prompt and caption from the summary input
SUMMARY_IMAGE_META_PROMPT='You are an assistant that extracts visual meaning from text. Given any descriptive input, generate:
1. "image_prompt" - a detailed, visual description suitable for generating an image.
2. "caption" - a brief, expressive caption that summarizes or enhances the image concept. No colons.
Return your response as a JSON object in the format: {"caption": "This is the generated caption", "image_prompt": "This is the image prompt"}.'
SUMMARY_ARTWORK_DETAILS_PROMPT="From the following text, extract any described artwork details and return a JSON object with:
\"details\": A detailed summary including the artwork's title, artist, date, medium, style, current location of artwork (location) and subject. Fill values with empty string if not specified in the text.
\"filename\": A lowercase alphanumeric string (no hyphens or underscores), 20 characters or fewer, representing a mostly unique filename derived from the title, artist, and date.
\"caption\": A caption (in proper English), no more than 20 words, that uses title, artist, date,location, medium and subject (in that ordered priority).
If no artwork is described in the text, return an empty JSON object {}."
# Prompt for augmenting artwork details
ART_DETAILS_AUGMENT_PROMPT="You are an art historian AI. Enhance the following JSON object with:
- original_title in the original language (if not present)
- title_language and ISO code
- a 20-word caption summarizing the artwork.
Return a well-formatted JSON object with the new fields added. Do not add any explanations."

# list of files for generating README and readthedocs documentation
# in the order below
CONTENT_DOCS=(
  "RIAY=start.md"
  "January=January.md"
  "February=February.md"
  "March=March.md"
  "April=April.md"
  "May=May.md"
  "June=June.md"
  "July=July.md"
  "August=August.md"
  "September=September.md"
  "October=October.md"
  "November=November.md"
  "December=December.md"
)

# image generation configuration
# turn on or off image generation
AUTO_GENERATE_IMAGES=false
# relative path to image generation script passed just one parameter --- the prompt ---
# and writing the path of the jpeg image generated to stdout
# you can plug in your own script here if you wish to use a different image generation engine and/or provider
IMAGE_GENERATION_SCRIPT="openaigenerateimage"
# deepinfra image generation inference model
DEEPINFRA_IMAGE_GENERATION_MODEL="stabilityai/sd3.5"
# falai image generation inference model
FALAI_IMAGE_GENERATION_MODEL="janus"

# art downloader configuration
AUTO_DOWNLOAD_ART=false
ART_DOWNLOADER_DIR="artdownloads"
MIN_IMAGE_WIDTH=350
MIN_IMAGE_HEIGHT=480
SEARCH_WIKIPEDIA=true
STOCK_PHOTO_SITES=(
  "alamy.com"
  "gettyimages.com"
  "gettyimages.co.uk"
  "istockphoto.com"
  "shutterstock.com"
  "dreamstime.com"
  "123rf.com"
  "depositphotos.com"
  "fineartamerica.com"
  "pixels.com"
  "bigstockphoto.com"
  "fotolia.com"
  "stock.adobe.com"
  "canstockphoto.com"
  "picfair.com"
  "granger.com"
  "bridgemanimages.com"
  "agefotostock.com"
  "europosters.nl"
  "nikkel-art.be"
  "etsy.com"
  "pixers.us"
  "ebayimg.com"
)
SOCIAL_MEDIA_SITES=(
  "reddit.com"
  "redd.it"
  "facebook.com"
  "X.com"
  "twitter.com"
  "tiktok.com"
  "threads.com"
  "snapchat.com"
  "linkedin.com"
  "pinterest.com"
  "quora.com"
  "tumblr.com"
  "4chan.com"
  "8kun.com"
  "imgur.com"
)

# art verifier settings
# parameter whether to verify art images or use rule-of-thumb where wikimedia images are preferred to images from google and duckduckgo
VERIFY_ART_IMAGES=true
# script that verifies art image
ART_VERIFIER_SCRIPT="matchimagetometadata.py"
# prompt to obtain art metadata
ART_METADATA_PROMPT="You are an expert on Christian art.
Provided the following details about the artwork '{}'
analyze the image and generate the following detailed metadata in json format (no markdown, no nesting of attributes, all top-level):
\"title\": The title of the artwork,
\"artist\": The artist or artists,
\"medium\": oil on canvas, fresco, marble sculpture, etc.,
\"location\": where the artwork is currently located,
\"date\": the creation year and century,
\"style\": the artistic style or artistic school that influenced the art,
\"description\": Description of the artwork (including visual elements, composition, subject matter, and style).
\"image_color\": Whether the image is in Color, Grayscale, Monochrome, Duotone/Tritone, Sepia,Color-tinted grayscale, Black-and-white etc.
\"watermarked\": Whether the image is watermarked or not.
\"caption\": A caption (in proper English), no more than 20 words, that uses title, artist, date, location, medium and description (in that ordered priority).
\"analyzed\": Whether the analysis was possible or not.
\"comments:\": Your comments other than the fields above and if analysis was possible or not and why.
Do not add any extraneous information that will mangle the json object expected."
# parameter to decide whether to allow rejected images to be embedded. Takes two values, strict or lenient
IMAGE_CONTENT_VALIDATION="lenient"
# vector embeddings model api key
VECTOR_EMBEDDINGS_MODEL_API_KEY="$DEEPINFRA_TOKEN"
# vector embeddings provider base url
VECTOR_EMBEDDINGS_BASE_URL="https://api.deepinfra.com/v1/openai"
# vector embeddings model used for cosine similarity
# if the model is used only to compare in memory
# you could use different models for production and test
# but if stored in file or database, use same models if files or databases shared
VECTOR_EMBEDDINGS_MODEL="thenlper/gte-large"
# Whether to look for alternate images using Google Lens
FIND_ALTERNATE_IMAGES=false
