#!/usr/bin/env bash
# Summarizes YouTube videos by extracting captions and using AI APIs to generate summaries
# Outputs markdown formatted summary

set -euo pipefail
shopt -s inherit_errexit

readonly VERSION="1.0.0"
readonly SCRIPT_NAME="$(basename "$0")"

# Source utility libraries
readonly SCRIPT_DIR="$(cd "$(dirname "${BASH_SOURCE[0]}")" &> /dev/null && pwd -P)"
source "${SCRIPT_DIR}/lib/require.sh"
source "${SCRIPT_DIR}/lib/internet.sh"
source "${SCRIPT_DIR}/lib/util.sh"
source "${SCRIPT_DIR}/lib/curl.sh"
source "${SCRIPT_DIR}/lib/youtube.sh"
source "${SCRIPT_DIR}/lib/random.sh"
source "${SCRIPT_DIR}/lib/lockconfig.sh"
lockconfig::lock_config_vars "${SCRIPT_DIR}/config.env"

function version() {
  printf "%s\n" "$VERSION"
}

function usage() {
  local exit_code=${1:-0}
  local output=${exit_code:-1}
  if [[ $exit_code -eq 0 ]]; then
    output=1
  else
    output=2
  fi

  >&"$output" cat << EOF
Usage: ${SCRIPT_NAME} [OPTIONS] <youtube-video-id>

Options:
  -d, --debug             Enable debug output
  --verbose               Enable verbose output
  --version               Show version information
  --help                  Show this help message

Environment variables required:
  TEXT_LLM_API_KEY           API key for LLM service
  TEXT_LLM_MODEL             text llm model
  TEXT_LLM_BASE_URL           text llm base url
  TEXT_LLM_CHAT_ENDPOINT      text llm chat endpoint
  SUMMARY_IMAGE_META_PROMPT  System prompt for image prompt and caption generation
  TEMPERATURE                Creativity parameter (0-2)
  TOP_P                      The model keeps only the smallest set of tokens whose cumulative probability mass is â‰¤ top_p.
                             Then it samples from that reduced set.
                             This is called nucleus sampling

Examples:
  ${SCRIPT_NAME} dQw4w9WgXcQ
EOF

  exit "$exit_code"
}

function get_chunk_prompt() {
  local chunk="$1"
  printf "%s\n\n%s\n" "${CHUNK_SUMMARY_PROMPT}" "${chunk}"
}

function get_preprocess_chunk_prompt() {
  local chunk="$1"
  printf "%s\n\n%s\n" "${PREPROCESS_CHUNK_PROMPT}" "${chunk}"
}

function get_final_prompt() {
  local input="$1"
  local final_prompt="${FINAL_SUMMARY_PROMPT}"
  local random_length="$(random::random_int 10 30)"
  new_final_prompt="${final_prompt//<no>/$random_length}"
  printf "%s\n\n%s\n" "${new_final_prompt}" "${input}"
}

function preprocess_chunk_llm() {
  local chunk="$1"
  local prompt="$(get_preprocess_chunk_prompt "$chunk")"
  local escaped_prompt="$(jq -Rs <<< "$prompt")"
  local escaped_system_prompt="$(jq -Rs <<< "$SYSTEM_PREPROCESS_PROMPT")"
  local payload="$(jq -n --arg system "$escaped_system_prompt" --arg content "$escaped_prompt" --arg model "${TEXT_LLM_MODEL}" --argjson temperature "$TEMPERATURE" --argjson top_p "$TOP_P" \
    '{
  "model": $model,
  "messages": [
    {
      "role": "system",
      "content": $system
    },
    {
      "role": "user",
      "content": $content
    }
  ],
  "temperature": $temperature,
  "top_p": $top_p
}')"

  local response
  response="$(
    curl::request \
      "${TEXT_LLM_BASE_URL}${TEXT_LLM_CHAT_ENDPOINT}" \
      "POST" \
      --header "Authorization: Bearer ${TEXT_LLM_API_KEY}" \
      --header "Content-Type: application/json" \
      --data "$payload"
  )"

  if ! echo "$response" | jq . > /dev/null 2>&1; then
    err "Text LLM API returned invalid response"
    return 1
  fi

  PREPROCESSED_CHUNK="$(echo "$response" | jq -r '.choices[0].message.content')"
}

function preprocess_chunk() {
  local chunk="$1"

  if [[ -n "${TEXT_LLM_API_KEY:-}" ]]; then
    if preprocess_chunk_llm "$chunk"; then
      return 0
    fi
  fi

  err "Failed to preprocess chunk (no working APIs available)"
  return 1
}

function summarize_chunk_llm() {
  local chunk="$1"
  local prompt="$(get_chunk_prompt "$chunk")"
  local escaped_prompt="$(jq -Rs <<< "$prompt")"
  local escaped_system_prompt="$(jq -Rs <<< "$SYSTEM_SUMMARY_PROMPT")"
  local payload="$(jq -n --arg system "$escaped_system_prompt" --arg content "$escaped_prompt" --arg model "${TEXT_LLM_MODEL}" --argjson temperature "$TEMPERATURE" --argjson top_p "$TOP_P" \
    '{
  "model": $model,
  "messages": [
    {
      "role": "system",
      "content": $system
    },
    {
      "role": "user",
      "content": $content
    }
  ],
  "temperature": $temperature,
  "top_p": $top_p
}')"

  local response
  response="$(
    curl::request \
      "${TEXT_LLM_BASE_URL}${TEXT_LLM_CHAT_ENDPOINT}" \
      "POST" \
      --header "Authorization: Bearer ${TEXT_LLM_API_KEY}" \
      --header "Content-Type: application/json" \
      --data "$payload"
  )"

  if ! echo "$response" | jq . > /dev/null 2>&1; then
    err "Text LLM API returned invalid response"
    return 1
  fi

  SUMMARIZED_CHUNK="$(echo "$response" | jq -r '.choices[0].message.content')"
}

function summarize_chunk() {
  local chunk="$1"

  if [[ -n "${TEXT_LLM_API_KEY:-}" ]]; then
    if summarize_chunk_llm "$chunk"; then
      return 0
    fi
  fi

  err "Failed to summarize chunk (no working APIs available)"
  return 1
}

function final_summary() {
  local input="$1"
  local prompt="$(get_final_prompt "$input")"
  local escaped_prompt="$(jq -Rs <<< "$prompt")"
  local escaped_system_prompt="$(jq -Rs <<< "$SYSTEM_SUMMARY_PROMPT")"
  local payload="$(
    jq -n \
      --arg system "$escaped_system_prompt" \
      --arg content "$escaped_prompt" \
      --arg model "$TEXT_LLM_MODEL" \
      --argjson temperature "$TEMPERATURE" \
      --argjson top_p "$TOP_P" \
      '{
      "model": $model,
      "messages": [
        {
          "role": "system",
          "content": $system
        },
        {
          "role": "user",
          "content": $content
        }
      ],
      "temperature": $temperature,
      "top_p": $top_p
    }'
  )"

  local response="$(
    curl::request \
      "${TEXT_LLM_BASE_URL}${TEXT_LLM_CHAT_ENDPOINT}" \
      "POST" \
      --header "Authorization: Bearer ${TEXT_LLM_API_KEY}" \
      --header "Content-Type: application/json" \
      --data "$payload"
  )"

  if ! echo "$response" | jq . > /dev/null 2>&1; then
    err "Text LLM API returned invalid response"
    return 1
  fi

  FINAL_SUMMARY="$(echo "$response" | jq -r '.choices[0].message.content')"
}

function main() {
  local verbose=false

  while [[ $# -gt 0 ]]; do
    case "$1" in
      -d | --debug)
        set -x
        shift
        ;;
      --version)
        version
        exit 0
        ;;
      --verbose)
        verbose=true
        shift
        ;;
      --help) usage 0 ;;
      --)
        shift
        break
        ;;
      -*) usage 1 ;;
      *) break ;;
    esac
  done

  if [[ $# -eq 0 ]]; then
    usage 1
  fi

  require_vars YOUTUBE_API_KEY CAPTIONS_OUTPUT_DIR TEXT_LLM_MODEL TEXT_LLM_API_KEY TEXT_LLM_BASE_URL TEXT_LLM_CHAT_ENDPOINT \
    TRANSCRIBE_VIDEOS TEMPERATURE TOP_P PREPROCESS_CHUNK_PROMPT CHUNK_SUMMARY_PROMPT FINAL_SUMMARY_PROMPT SYSTEM_SUMMARY_PROMPT \
    SYSTEM_PREPROCESS_PROMPT CHUNK_SIZE

  local VIDEO_ID="$1"
  local captions_file="${CAPTIONS_OUTPUT_DIR}/${VIDEO_ID}.en.txt"

  if "${TRANSCRIBE_VIDEOS:-false}"; then
    if ! "${SCRIPT_DIR}/transcribevideo" "$VIDEO_ID" -o "$captions_file"; then
      die "Error transcribing captions for ${VIDEO_ID}"
    fi
  else
    if ! vtt_file="$("${SCRIPT_DIR}/downloadcaptions" -- "$VIDEO_ID")"; then
      if "${ENABLE_FAILOVER_MODE:-true}"; then
        if ! "${SCRIPT_DIR}/transcribevideo" "$VIDEO_ID" -o "$captions_file"; then
          die "Error transcribing captions for ${VIDEO_ID}"
        fi
      else
        die "Error downloading captions for ${VIDEO_ID}"
      fi
    else
      if ! captions_file="$("${SCRIPT_DIR}/extractcaptions" "$vtt_file")"; then
        die "Error extracting captions from ${vtt_file}"
      fi
    fi
  fi

  local TRANSCRIPT
  TRANSCRIPT="$(cat "$captions_file")"

  if [[ -z "$TRANSCRIPT" ]]; then
    die "Error: Transcript conversion failed"
  fi

  >&2 echo "Transcript extracted (length: ${#TRANSCRIPT} chars)"

  local TOTAL_LEN=${#TRANSCRIPT}
  local CHUNKS=()
  for ((i = 0; i < TOTAL_LEN; i += CHUNK_SIZE)); do
    CHUNKS+=("${TRANSCRIPT:i:CHUNK_SIZE}")
  done

  >&2 echo "Split transcript into ${#CHUNKS[@]} chunks"

  local INTERMEDIATE_SUMMARIES=()
  local start_time=$(date +%s)
  declare -i chunk_num=1
  for chunk in "${CHUNKS[@]}"; do
    local PREPROCESSED_CHUNK
    preprocess_chunk "$chunk"
    >&2 echo "Preprocessed chunk ${chunk_num} (length: ${#chunk} chars)"
    local SUMMARIZED_CHUNK
    summarize_chunk "$PREPROCESSED_CHUNK"
    INTERMEDIATE_SUMMARIES+=("${SUMMARIZED_CHUNK}")
    >&2 echo "Summarized chunk ${chunk_num} (length: ${#PREPROCESSED_CHUNK} chars)"
    PREPROCESSED_CHUNK=""
    SUMMARIZED_CHUNK=""
    if [[ "${GAP_BW_REQS}" -gt 0 ]]; then
      >&2 echo "Waiting for ${GAP_BW_REQS} seconds before initiating next request...."
      sleep "$GAP_BW_REQS"
    fi
    ((chunk_num++))
  done

  local FINAL_INPUT
  FINAL_INPUT=$(printf "%s\n\n" "${INTERMEDIATE_SUMMARIES[@]}")

  local FINAL_SUMMARY=""

  final_summary "$FINAL_INPUT"

  local end_time=$(date +%s)
  local elapsed_time=$((end_time - start_time))

  if [[ -z "$FINAL_SUMMARY" ]]; then
    die "ERROR: Failed to generate final summary (no working API available)"
  fi

  >&2 echo "Final summary generated successfully"

  >&2 echo "Summarized captions in ${elapsed_time} seconds"
  echo -e "${FINAL_SUMMARY}" | sed -E '/^```(markdown)?[[:space:]]*$/d'
}

if [[ "${BASH_SOURCE[0]}" == "${0}" ]]; then
  require_commands date jq tee cat
  if "${LOGGING:-false}"; then
    timestamp=$(date +"%Y%m%d_%H%M%S")
    {
      main "$@" 2> >(tee -a "${SCRIPT_NAME%.*}_${timestamp}.stderr.log" >&2)
    } | tee -a "${SCRIPT_NAME%.*}_${timestamp}.stdout.log"
  else
    main "$@"
  fi
fi
